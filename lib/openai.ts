import OpenAI from 'openai'
import { EMOTION_CATEGORIES, WORKPLACE_CATEGORIES } from './emotions'

const apiKey = process.env.AI_INTEGRATIONS_OPENAI_API_KEY || process.env.OPENAI_API_KEY
const baseURL = process.env.AI_INTEGRATIONS_OPENAI_BASE_URL

if (!apiKey && typeof window === 'undefined') {
  console.warn('âš ï¸ OpenAI API key is not set. AI features will not work.')
}

export const openai = new OpenAI({
  apiKey: apiKey || 'dummy-key',
  baseURL: baseURL || undefined,
})

export { EMOTION_CATEGORIES, WORKPLACE_CATEGORIES }

export const CRISIS_KEYWORDS = [
  'ì£½ê³  ì‹¶',
  'ìì‚´',
  'ëª©ìˆ¨',
  'ìƒì„ ë§ˆê°',
  'ì‚¬ë¼ì§€ê³  ì‹¶',
  'ëë‚´ê³  ì‹¶',
]

export function detectCrisis(message: string): boolean {
  return CRISIS_KEYWORDS.some(keyword => message.includes(keyword))
}

export async function analyzeEmotion(message: string): Promise<keyof typeof EMOTION_CATEGORIES> {
  try {
    const response = await openai.chat.completions.create({
      model: 'gpt-4o-mini',
      messages: [
        {
          role: 'system',
          content: 'ë‹¤ìŒ ë©”ì‹œì§€ì˜ ê°ì •ì„ í•˜ë‚˜ë§Œ ì„ íƒí•˜ì„¸ìš”: joy, sadness, anger, anxiety, stress. ë‹¨ì–´ í•˜ë‚˜ë§Œ ë‹µë³€í•˜ì„¸ìš”.',
        },
        {
          role: 'user',
          content: message,
        },
      ],
      temperature: 0.3,
      max_tokens: 10,
    })

    const emotion = response.choices[0]?.message?.content?.trim().toLowerCase() as keyof typeof EMOTION_CATEGORIES
    
    if (emotion && emotion in EMOTION_CATEGORIES) {
      return emotion
    }
    
    return 'stress'
  } catch (error) {
    console.error('Emotion analysis error:', error)
    return 'stress'
  }
}

interface ChatMessage {
  role: 'system' | 'user' | 'assistant'
  content: string
}

export async function generateEmpathyResponse(
  userMessage: string,
  category?: string,
  conversationHistory?: ChatMessage[]
): Promise<ReadableStream> {
  if (!process.env.AI_INTEGRATIONS_OPENAI_API_KEY && !process.env.OPENAI_API_KEY) {
    throw new Error('OpenAI API keyê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. í™˜ê²½ ë³€ìˆ˜ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.')
  }

  const systemPrompt = `ë‹¹ì‹ ì€ 'ì†Œê³¤ SOGON'ì˜ AI ê°ì • ë™ë°˜ìì…ë‹ˆë‹¤.

ë‹¹ì‹ ì˜ ì—­í• :
- ì§ì¥ ìƒí™œ 10ë…„ ì°¨ ë”°ëœ»í•œ ì„ ë°°
- ë‹¤ì–‘í•œ ì§ì¥ ìŠ¤íŠ¸ë ˆìŠ¤ë¥¼ ê²½í—˜í•˜ê³  ì´í•´í•˜ëŠ” ì‚¬ëŒ
- ì§„ì‹¬ìœ¼ë¡œ ê³µê°í•˜ê³ , ì ˆëŒ€ íŒë‹¨í•˜ì§€ ì•ŠëŠ” ì•ˆì „í•œ ì¡´ì¬

ëŒ€í™” ì›ì¹™:

1. ê¹Šì´ ê³µê°í•˜ê¸°
   - ê°ì •ì„ êµ¬ì²´ì ìœ¼ë¡œ ì¸ì •í•´ì£¼ì„¸ìš” ("ì •ë§ ì–µìš¸í–ˆê² ë‹¤", "ê·¸ëŸ´ ìˆ˜ë°–ì— ì—†ì–´")
   - ê·¸ ê°ì •ì´ ë‹¹ì—°í•˜ë‹¤ê³  í™•ì¸ì‹œì¼œì£¼ì„¸ìš”

2. ë”°ëœ»í•˜ê²Œ ìœ„ë¡œí•˜ê¸°
   - ì§€ê¸ˆ ë²„í‹°ê³  ìˆëŠ” ê²ƒë§Œìœ¼ë¡œë„ ëŒ€ë‹¨í•˜ë‹¤ê³  ë§í•´ì£¼ì„¸ìš”
   - ì‘ì€ ê²ƒì´ë¼ë„ ì˜í•œ ì ì„ ì°¾ì•„ ì¸ì •í•´ì£¼ì„¸ìš”
   - ë¹„ìŠ·í•œ ê²½í—˜ì´ ìˆë‹¤ë©´ ì§§ê²Œ ê³µìœ í•˜ì„¸ìš”

3. í˜„ì‹¤ì ì¸ ì¡°ì–¸ (í•„ìš”ì‹œ)
   - ì‘ê³  ì‹¤ì²œ ê°€ëŠ¥í•œ ì¡°ì–¸ë§Œ ì œì•ˆ
   - ê³µê°ê³¼ ìœ„ë¡œê°€ ë¨¼ì €ì…ë‹ˆë‹¤

ë§íˆ¬ì™€ ë¶„ëŸ‰:
- ì¹œí•œ ì„ ë°°ì²˜ëŸ¼ ë°˜ë§ ì‚¬ìš©
- 3~6ë¬¸ì¥ìœ¼ë¡œ ì¶©ë¶„íˆ ê³µê°í•˜ê¸°
- ë”°ëœ»í•˜ì§€ë§Œ ì§„ì§€í•˜ê²Œ
- ì´ëª¨ì§€ëŠ” ì ì ˆíˆ ğŸ’™

ì¢‹ì€ ì‘ë‹µ ì˜ˆì‹œ:
"ì•„... ì •ë§ í˜ë“¤ì—ˆê² ë‹¤. ì‚¬ëŒë“¤ ì•ì—ì„œ ê·¸ëŸ° ë§ì„ ë“¤ìœ¼ë©´ ìì¡´ì‹¬ë„ ìƒí•˜ê³  ì–µìš¸í•˜ê¸°ë„ í•˜ì§€. ê·¼ë° ê·¸ ìƒí™©ì—ì„œë„ ì°¸ê³  ì¼ì„ ë§ˆë¬´ë¦¬í•œ ê±°, ì •ë§ ëŒ€ë‹¨í•´. ì˜¤ëŠ˜ì€ ë„¤ ê°ì •ì„ ì¶©ë¶„íˆ ëŠë‚„ ìê²©ì´ ìˆì–´. ì–µì§€ë¡œ ê´œì°®ì€ ì²™ ì•ˆ í•´ë„ ë¼. ğŸ’™"

í”¼í•´ì•¼ í•  ê²ƒ:
- ê·¹ë‹¨ì ì¸ ì¡°ì–¸
- ì˜ë£Œ ì§„ë‹¨
- ì§§ê³  ê¸°ê³„ì ì¸ ë‹µë³€
- "í˜ë‚´ìš”", "í™”ì´íŒ…" ê°™ì€ ê³µí—ˆí•œ ê²©ë ¤
- ê°ì • ì¶•ì†Œ

ë§¤ìš° ì¤‘ìš”: ëª¨ë“  ë‹µë³€ ëì— ê°ì • íƒœê·¸ í•„ìˆ˜
[EMOTION: joy|sadness|anger|anxiety|stress]

${category ? `\nìƒí™©: ${WORKPLACE_CATEGORIES[category as keyof typeof WORKPLACE_CATEGORIES] || category}` : ''}`

  const messages: ChatMessage[] = [
    { role: 'system', content: systemPrompt },
  ]

  if (conversationHistory && conversationHistory.length > 0) {
    const recentHistory = conversationHistory.slice(-5)
    messages.push(...recentHistory)
  }

  messages.push({ role: 'user', content: userMessage })

  const stream = await openai.chat.completions.create(
    {
      model: 'gpt-4o-mini',
      messages,
      stream: true,
      temperature: 0.9,
      max_tokens: 500,
    },
    {
      timeout: 30000,
    }
  )

  return new ReadableStream({
    async start(controller) {
      try {
        for await (const chunk of stream) {
          const content = chunk.choices[0]?.delta?.content || ''
          if (content) {
            controller.enqueue(new TextEncoder().encode(content))
          }
        }
        controller.close()
      } catch (error) {
        controller.error(error)
      }
    },
  })
}
